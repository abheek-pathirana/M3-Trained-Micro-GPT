# M3-Trained-Micro-GPT
A 5.6M param GPT-2-style LLM pre-trained from scratch on an Apple M3 MacBook Air using 8.1M tokens and 70 epochs (6.41 petaFLOPs). Shows itâ€™s possible to pre-train tiny LLMs on consumer hardware using PyTorch + FP32. Includes model, configs, training code, and samples.
